{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for Character Recognition\n",
    "\n",
    "**Recognizing handwritten digits!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching a computer to learn\n",
    "\n",
    "This is a typical example of a classification problem, where handwritten test images are classified into one of the 9 classes *[0-9]*. Before actually solving the problem, we discuss other yet simpler examples\n",
    "to understand what machine learning is all about.\n",
    "\n",
    "### Classify furniture\n",
    "\n",
    "Consider a problem of identifying from a set of furniture based on the dimensions, whether it is a chair, a table or a bed. \n",
    "\n",
    "<img src=\"./furn1.jpg\" width=\"450\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the observation from the above graph shows that $(W_{chair} + H_{chair}) < (W_{table} + H_{table})$. \n",
    "We can design a neural network as shown below such that the right neuron fires based on the inputs $W$ and $H$.\n",
    "\n",
    "<img src=\"./furn2.jpg\" width=\"450\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there might be ambiguities when dealing with bed and tables, the summations might not be very different. We then use the $weight$ parameters to scale down certain features. Another variable parameter is the $threshold$ which decides if a neuron will fire or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Neural Network Learning\n",
    "\n",
    "As suggested Neural Networks Learn from large number of real life instances and acquire knowledge which is used to make predictions.\n",
    "\n",
    "<img src=\"./nn1.jpg\" width=\"450\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a very high level the process of learning can be thought of as,\n",
    "\n",
    "<img src=\"./nn2.jpg\" width=\"450\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "function sigm(a)\n",
    "    1. / (1. + exp(-a))\n",
    "end\n",
    "p = plot(sigm,-5,5,Guide.title(\"Sigmoidal activation function\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Descent \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning algorithms sound terrific. But how can we devise such algorithms for a neural network? Suppose we have a network of perceptrons that we'd like to use to learn to solve some problem. For example, the inputs to the network might be the raw pixel data from a scanned, handwritten image of a digit. And we'd like the network to learn weights and biases so that the output from the network correctly classifies the digit. To see how learning might work, suppose we make a small change in some weight (or bias) in the network. What we'd like is for this small change in weight to cause only a small corresponding change in the output from the network. As we'll see in a moment, this property will make learning possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "<img src=\"./gate1.jpg\" width=\"300\" align=\"left\"/>\n",
    "\n",
    "Consider a simple circuit which takes 2 inputs `x`, `y` and outputs its product `x*y`. \n",
    "\n",
    "$$f(x, y) = x*y$$\n",
    "\n",
    "Now let us try to find a way to **increase the output slightly by tweaking the inputs slightly**.\n",
    "\n",
    "Guessing some random values `x` = -1.99 and `y` = `2.99`, results in `x * y = -5.95`, which meets our objective of slight increase in output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random search method might work on small data, but when the dimensions get bigger it is not practical to use. Another way is to imagine the desired change in the output, i.e., increase is slightly and intuit the forces on `x` and `y` which will aid this change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = -2 \n",
    "y = 3\n",
    "forwardMultiplyGate(x, y) = x*y\n",
    "\n",
    "forwardMultiplyGate(-2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forwardMultiplyGate(-1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change in output might be in the right direction, but the magnitude is bit too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forwardMultiplyGate(-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = -2;\n",
    "y = 3;\n",
    "out = forwardMultiplyGate(x, y); # before: -6\n",
    "x_gradient = y; # by our complex mathematical derivation above\n",
    "y_gradient = x;\n",
    "\n",
    "step_size = 0.01;\n",
    "x += step_size * x_gradient; # -2.03\n",
    "y += step_size * y_gradient; # 2.98\n",
    "out_new = forwardMultiplyGate(x, y); # -5.87. Higher output! Nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above example helps us understand how partial derivatives help in calculating the **analytical gradient** which is widely used in defining the **Cost function** of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous gate was very simple and the derivative too was straight forward. Let us add some complexity,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./gate2.jpg\" width=\"300\" align=\"left\"/>\n",
    "\n",
    "This is a combination of 2 gates, \n",
    "\n",
    "$$f(x,y,z) = (x + y) z$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "function forwardMultiplyGate(a, b) \n",
    "  return a * b;\n",
    "end;\n",
    "    \n",
    "function forwardAddGate(a, b) \n",
    "  return a + b;\n",
    "end;\n",
    "        \n",
    "function forwardCircuit(x,y,z) \n",
    "  q = forwardAddGate(x, y);\n",
    "  r = forwardMultiplyGate(q, z);\n",
    "  return r;\n",
    "end\n",
    "\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4;\n",
    "f2 = forwardCircuit(x, y, z); # output is -12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function forwardMultiplyGate(a, b) \n",
    "  return a * b;\n",
    "end\n",
    "\n",
    "function forwardAddGate(a, b) \n",
    "  return a + b;\n",
    "end;\n",
    "\n",
    "function forwardCircuit(x,y,z)\n",
    "  q = forwardAddGate(x, y);\n",
    "  f = forwardMultiplyGate(q, z);\n",
    "  return f;\n",
    "end;\n",
    "\n",
    "x = -2;\n",
    "y = 5;\n",
    "z = -4;\n",
    "f = forwardCircuit(x, y, z); # output is -12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial conditions\n",
    "x = -2;\n",
    "y = 5;\n",
    "z = -4;\n",
    "q = forwardAddGate(x, y); # q is 3\n",
    "f = forwardMultiplyGate(q, z); # output is -12\n",
    "\n",
    "# gradient of the MULTIPLY gate with respect to its inputs\n",
    "# wrt is short for \"with respect to\"\n",
    "derivative_f_wrt_z = q; # 3\n",
    "derivative_f_wrt_q = z; # -4\n",
    "\n",
    "# derivative of the ADD gate with respect to its inputs\n",
    "derivative_q_wrt_x = 1.0;\n",
    "derivative_q_wrt_y = 1.0;\n",
    "\n",
    "# chain rule\n",
    "derivative_f_wrt_x = derivative_q_wrt_x * derivative_f_wrt_q; # -4\n",
    "derivative_f_wrt_y = derivative_q_wrt_y * derivative_f_wrt_q; # -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient_f_wrt_xyz = [derivative_f_wrt_x, derivative_f_wrt_y, derivative_f_wrt_z]\n",
    "\n",
    "# let the inputs respond to the force/tug:\n",
    "step_size = 0.01;\n",
    "x = x + step_size * derivative_f_wrt_x; # -2.04\n",
    "y = y + step_size * derivative_f_wrt_y; # 4.96\n",
    "z = z + step_size * derivative_f_wrt_z; # -3.97\n",
    "\n",
    "# Our circuit now better give higher output:\n",
    "q = forwardAddGate(x, y); # q becomes 2.92\n",
    "f = forwardMultiplyGate(q, z); # output is -11.59, up from -12! Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Recognition using Logistic regression\n",
    "\n",
    "This model is a good example to understand how a basic single layer neural network, let us read some digits from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MNIST\n",
    "\n",
    "trainX, trainY = traindata()\n",
    "testX, testY = testdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function shownum(X, i)\n",
    "    num = X[:,i]\n",
    "    num = reshape(num, 28,28)\n",
    "    \n",
    "    num = num'./maximum(num)\n",
    "    \n",
    "    return grayim(num)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Interact, Images\n",
    "sample = @manipulate for i=1:100; i; end\n",
    "\n",
    "display(map(i->shownum(trainX, i), sample))\n",
    "display(map(i->trainY[i], sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a softmax function `soft` which is a generalization of a logistix function, which is used as a activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soft(xs) = exp(xs)/sum(exp(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let the weights and the output be randomly initialised\n",
    "W = rand(10, 784);\n",
    "b = rand(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us define the neural network\n",
    "net(x) = soft(W*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us try to classify ith image\n",
    "i = 1\n",
    "y_out = net(normalize(trainX[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findmax(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network says that the digit is a `8` which is definitely wrong. Hence we modify the weights in such a way that the network learns how to classify the digit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "η = 0.01\n",
    "W = W .+ η\n",
    "\n",
    "# Let us try to classify ith image\n",
    "i = 1\n",
    "y_out = net(normalize(trainX[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findmax(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deep Learning in Julia\n",
    "\n",
    "What we just saw was a simplest 1 layer network, with softmax activation. It was apparent that the performance is not good, and we can stack up multiple such layers to gain better performance.\n",
    "\n",
    "We look into simple multi-layer perceptron model to train on MNIST data and later we show how to do the same using a convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple 3-Layer MLP\n",
    "\n",
    "<img src=\"./mlp2.jpg\" width=\"450\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXNet provides a symbolic API, which hides all the complexity of the network. Let us define the network architecture by starting with the data symbol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mx.Variable(:data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cascading fully-connected layers and activation functions\n",
    "fc1  = mx.FullyConnected(data = data, name=:fc1, num_hidden=128)\n",
    "act1 = mx.Activation(data = fc1, name=:relu1, act_type=:relu)\n",
    "fc2  = mx.FullyConnected(data = act1, name=:fc2, num_hidden=64)\n",
    "act2 = mx.Activation(data = fc2, name=:relu2, act_type=:relu)\n",
    "fc3  = mx.FullyConnected(data = act2, name=:fc3, num_hidden=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note each composition we take the previous symbol as the data argument, forming a feedforward chain. The architecture looks like,\n",
    "\n",
    "`Input --> 128 units (ReLU) --> 64 units (ReLU) --> 10 units`\n",
    "\n",
    "where the last 10 units correspond to the 10 output classes (digits 0,...,9). We then add a final SoftmaxOutput operation to turn the 10-dimensional prediction to proper probability values for the 10 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp  = mx.SoftmaxOutput(data = fc3, name=:softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the MLP is just a chain of layers. For this case, we can also use the mx.chain macro. The same architecture above can be defined as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = @mx.chain mx.Variable(:data)             =>\n",
    "  mx.FullyConnected(name=:fc1, num_hidden=128) =>\n",
    "  mx.Activation(name=:relu1, act_type=:relu)   =>\n",
    "  mx.FullyConnected(name=:fc2, num_hidden=64)  =>\n",
    "  mx.Activation(name=:relu2, act_type=:relu)   =>\n",
    "  mx.FullyConnected(name=:fc3, num_hidden=10)  =>\n",
    "  mx.SoftmaxOutput(name=:softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the architecture, we are ready to load the MNIST data. MXNet.jl provide built-in data providers for the MNIST dataset, which could automatically download the dataset into Pkg.dir(\"MXNet\")/data/mnist if necessary. We wrap the code to construct the data provider into mnist-data.jl so that it could be shared by both the MLP example and the LeNet ConvNets example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "include(joinpath(Pkg.dir(\"MXNet\"),\"examples\",\"mnist\",\"mnist-data.jl\"))\n",
    "train_provider, eval_provider = get_mnist_providers(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to write your own data providers for customized data format you can use `mx.AbstractDataProvider`. \n",
    "\n",
    "Data providers are wrappers that load external data, be it images, text, or general tensors, and split it into mini-batches so that the model can consume the data in a uniformed way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_provider.data_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the aboe result that we have picked 100 images from the training data, and each image is of length `784 = 28*28`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the architecture and data, we can instantiate an model to do the actual training. mx.FeedForward is the built-in model that is suitable for most feed-forward architectures. When constructing the model, we also specify the context on which the computation should be carried out. Because this is a really tiny MLP, we will just run on a single CPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = mx.FeedForward(mlp, context=mx.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a mx.gpu() or if a list of devices (e.g. [mx.gpu(0), mx.gpu(1)]) is provided, data-parallelization will be used automatically. But for this tiny example, using a GPU device might not help.\n",
    "\n",
    "The last thing we need to specify is the optimization algorithm (a.k.a. optimizer) to use. We use the basic SGD with a fixed learning rate 0.1 and momentum 0.9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = mx.SGD(lr=0.1, momentum=0.9, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the training. Here the n_epoch parameter specifies that we want to train for 20 epochs. We also supply a eval_data to monitor validation accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second example, we show a slightly more complicated architecture that involves convolution and pooling. This architecture for the MNIST is usually called the [LeNet]_. The first part of the architecture is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input\n",
    "data = mx.Variable(:data)\n",
    "\n",
    "# first conv\n",
    "conv1 = @mx.chain mx.Convolution(data=data, kernel=(5,5), num_filter=20)  =>\n",
    "                  mx.Activation(act_type=:tanh) =>\n",
    "                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))\n",
    "\n",
    "# second conv\n",
    "conv2 = @mx.chain mx.Convolution(data=conv1, kernel=(5,5), num_filter=50) =>\n",
    "                  mx.Activation(act_type=:tanh) =>\n",
    "                  mx.Pooling(pool_type=:max, kernel=(2,2), stride=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically defined two convolution modules. Each convolution module is actually a chain of Convolution, tanh activation and then max `Pooling` operations.\n",
    "\n",
    "Each sample in the MNIST dataset is a 28x28 single-channel grayscale image. In the tensor format used by `NDArray`, a batch of 100 samples is a tensor of shape `(28,28,1,100)`. The convolution and pooling operates in the spatial axis, so `kernel=(5,5)` indicate a square region of 5-width and 5-height. The rest of the architecture follows as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first fully-connected\n",
    "fc1   = @mx.chain mx.Flatten(data=conv2) =>\n",
    "                  mx.FullyConnected(num_hidden=500) =>\n",
    "                  mx.Activation(act_type=:tanh)\n",
    "\n",
    "# second fully-connected\n",
    "fc2   = mx.FullyConnected(data=fc1, num_hidden=10)\n",
    "\n",
    "# softmax loss\n",
    "lenet = mx.Softmax(data=fc2, name=:softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a fully-connected operator expects the input to be a matrix. However, the results from spatial convolution and pooling are 4D tensors. So we explicitly used a Flatten operator to flat the tensor, before connecting it to the FullyConnected operator.\n",
    "\n",
    "The rest of the network is the same as the previous MLP example. As before, we can now load the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "include(joinpath(Pkg.dir(\"MXNet\"),\"examples\",\"mnist\",\"mnist-data.jl\"))\n",
    "train_provider, eval_provider = get_mnist_providers(batch_size; flat=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we specified flat=false to tell the data provider to provide 4D tensors instead of 2D matrices because the convolution operators needs correct spatial shape information. We then construct a feedforward model on GPU, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model = mx.FeedForward(lenet, context=mx.cpu())\n",
    "\n",
    "# optimizer\n",
    "optimizer = mx.SGD(lr=0.05, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "# fit parameters\n",
    "mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Predicting with a trained model\n",
    "\n",
    "Predicting with a trained model is very simple. By calling `mx.predict` with the model and a data provider, we get the model output as a Julia Array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = mx.predict(model, eval_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = Array[]\n",
    "for batch in eval_provider\n",
    "    push!(labels, copy(mx.get(eval_provider, batch, :softmax_label)))\n",
    "end\n",
    "labels = cat(1, labels...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = @manipulate for i=1:100; i; end\n",
    "\n",
    "display(map(i->shownum(testX, i), sample))\n",
    "display(map(i->labels[i], sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "Int(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shownum(testX, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
